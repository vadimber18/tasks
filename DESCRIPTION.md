# tasks

## Общее описание
Архитектура состоит из двух сервисов - `сервис A` и `сервис B`. Сервисы общаются по `rabbitmq`

У первого имеется апи, по которому можно добавить, посмотреть и изменить статус таски. При добавлении в очередь закидывается сообщение с таской.

Второй сервис ловит сообщение из очереди, смотрит, какое оно. Если `cpu-bound` - отправляет в `celery` (который тоже подключен к `rabbitmq`, потому что он у нас уже есть), иначе - выполняет асинхронно сам.

После начала выполнения `сервис B` дергает `сервис A` по http и изменяет статус таски. По завершению или ошибке - делает это еще раз.

Сделал минимальные тесты, подключил линтеры. Это все запускается через мейкфайл через докер-композ (решение так себе, но для этого задания норм, в реальной жизни это просто все прокидывается в `gitlab-ci.yaml` и либо на локальной машине эмулируется гитлаб, либо смотрим по фича-ветке). Заставлять тебя создавать `venv` и что-то делать руками тоже так себе идея.

Взаимодействие с базой асинхронное, `alembic` миграции с [микро-докой](https://github.com/vadimber18/tasks/blob/master/service/db_model/alembic/README)

Есть повторяемый код, но в реальной жизни разные сервисы живут в разных репах, поэтому его было бы меньше. Шарить одну `common` папку не хочу.

Сделал минимальное логирование, в реальной жизни, опять же, в файлы никто не пишет (если только настроить файлбит, который в логстеш будет закидывать), надо делать через логстеш напрямую либо через очереди)


Схема работы сервиса

![system_design1](https://user-images.githubusercontent.com/17683944/156919199-b314c499-36c7-4a23-a4ca-e0bbf8ee8b95.png)



## Минусы

[database per service](https://microservices.io/patterns/data/database-per-service.html) не нарушаем, зато некрасиво из второго сервиса к первому обращаться по `http` (но так указано сделать в задании, видимо так надо). Лучше это сделать через очередь, потому что если первый сервис упадет - то сообщение потеряется и `consistency` испортится, а если отправим в очередь - сервис сообщение подхватит.


## Как сделать лучше:

Сделать отдельный сервис, функцией которого является только взаимодействие с базой (или хранилищем) мета-информации о тасках. К нему можно сделать доступ по `json-rpc`, а сам сервис будет работать с монгой, это вероятно лучше решение, если нагрузка большая и\или данных много. Но здесь можно выстрелить себе в ногу, если прям в этом сервисе мы захотим джойнить таски со сложной логикой. Есть конечно [aggregation framework](https://docs.mongodb.com/manual/aggregation/), но он может ударить по оперативке и по быстродействию, поэтому конечно такие базы обычно юзают, когда нет четкой схемы и связи не нужны (по крайней мере с другими базами). Если нагрузка небольшая, то все ок, просто юзаем этот сервис, а агрегируем данные где-то еще. Если боимся за рейс кондишн - ставим очередь, потеряем консистенси, ну и ладно, назовем это [eventual consistency](https://en.wikipedia.org/wiki/Eventual_consistency#:~:text=Eventual%20consistency%20is%20a%20consistency,return%20the%20last%20updated%20value.), мы такое видим каждый день везде.

Второй вариант - про который я говорил в "минусах" - закидываем сообщение от `сервиса B` к `сервису A` в очередь (если вариант с `json-rpc` сервисом, подключенным к шине, не нравится). Трейд офф будет, зато `availability` норм

Схема работы нормальной архитектуры (с вариантами)

![system_design2](https://user-images.githubusercontent.com/17683944/156892232-de597c51-ffe8-451a-9ece-48afeecbf752.png)