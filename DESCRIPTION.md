Общее описание
"Архитектура" состоит из двух сервисов - сервис А и сервис В
У первого имеется апи, по которому можно добавить, посмотреть и изменить статус таски. При добавлении он шлет в rabbitmq сообщение.
Второй сервис ловит сообщение из очереди, смотрит, какое оно. Если cpu-bound - отправляет в селери (который тоже подключен к rabbitmq, потому что он у нас уже есть), иначе - выполняет асинхронно сам.
После начала выполнения сервис В дергает сервис А по http и изменяет статус таски. По завершению или ошибке - делает это еще раз.
Сделал минимальные тесты, подключил линтеры. Это все запускается через мейкфайл через докер-композ (решение так себе, но для этого задания норм, в реальной жизни это просто все прокидывается в gitlab-ci.yaml и либо на локальной машине эмулируется гитлаб, либо смотрим по фича-ветке). Заставлять тебя создавать venv и что-то делать руками неохота.
Взаимодействие с базой асинхронное, алембик миграции, есть микро-ридми(ссылка) как их юзать
Есть повторяемый код, но в реальной жизни разные сервисы живут в разных репах, поэтому тут это норм
Сделал минимальное логирование, в реальной жизни, опять же, в файлы никто не пишет (если только настроить файлбит, который в логстеш будет закидывать), надо делать через логстеш напрямую либо через очереди)


Схема работы сервиса

![system_design1](https://user-images.githubusercontent.com/17683944/156892225-26fbd867-3db3-4fc0-adfa-ee5dea1f3b51.png)



Минусы:
database per service не нарушаем, зато некрасиво из второго сервиса к первому обращаться по http (так указано сделать в задании, видимо так надо). Лучше это сделать через очередь, потому что если первый сервис упадет - то сообщение потеряется и consistency испортится, а если отправим в очередь - сервис сообщение подхватит.

Как сделать лучше:
Еще лучше сделать отдельный сервис, функцией которого является только взаимодействие с базой (или хранилищем) мета-информации о тасках. К нему можно сделать доступ по json-rpc, а сам сервис будет работать с монгой, это вероятно лучше решение, если нагрузка большая и\или данных много. Но здесь можно выстрелить себе в ногу, если прям в этом сервисе мы захотим джойнить таски со сложной логикой. Есть конечно aggregation framework, но он может ударить по оперативке и по быстродействию, поэтому конечно такие базы обычно юзают, когда нет четкой схемы и связи не нужны (по крайней мере с другими базами). Если нагрузка небольшая, то все ок, просто юзаем этот сервис, а агрегируем данные где-то еще. Если боимся за рейс кондишн - ставим очередь, потеряем консистенси, ну и ладно, назовем это eventual consistency, мы такое видем каждый день везде.
Второй вариант - про который я говорил в "минусах" - закидываем сообщение от сервиса В к сервису А в очередь. Трейд офф будет, зато availability норм

Схема работы нормальной архитектуры (с вариантами)

![system_design2](https://user-images.githubusercontent.com/17683944/156892232-de597c51-ffe8-451a-9ece-48afeecbf752.png)